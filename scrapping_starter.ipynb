{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using python\n",
    "\n",
    "#### References\n",
    "1. [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "2. [Web Scraping using Python](https://www.datacamp.com/community/tutorials/web-scraping-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ python3 -m venv venv\n",
    "# $ . ./venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: fire in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (1.9.5)\n",
      "Requirement already satisfied: termcolor in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from fire) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\tijesunimi2-pc\\anaconda3\\lib\\site-packages (from fire) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "#Better\n",
    "!pip install requests BeautifulSoup4 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content  #.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "def get_element(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "    if response is not None:\n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "        if search:\n",
    "            soup = html            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('findaing',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "        return res\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "if get_ipython().__class__.__name__ == '__main__':\n",
    "    fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrape 100 most influential Twitter users in Africa using Python or Bash to obtain the 100 African twitter influencers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100. Jeffrey Gettleman (@gettleman)',\n",
       " '99. Africa24 Media (@a24media)',\n",
       " '98. Scapegoat (@andiMakinana)',\n",
       " '97. Africa Check (@AfricaCheck)',\n",
       " '96. James Copnall (@JamesCopnall)',\n",
       " '95. Online Africa (@oafrica)',\n",
       " '94. Patrick Ngowi (@PatrickNgowi)',\n",
       " '93. DOS African Affairs (@StateAfrica)',\n",
       " '92. MoadowAJE (@Moadow)',\n",
       " '91. Brendan Boyle (@BrendanSAfrica)',\n",
       " '90. City of Tshwane (@CityTshwane)',\n",
       " '89. VISI Magazine (@VISI_Mag)',\n",
       " '88. andBeyond (@andBeyondSafari)',\n",
       " '87. This Is Africa (@ThisIsAfricaTIA)',\n",
       " '86. Sarah Carter (@sarzss)',\n",
       " '85. The EIU Africa team (@TheEIU_Africa)',\n",
       " '84. Investing In Africa (@InvestInAfrica)',\n",
       " '83. Barry Malone (@malonebarry)',\n",
       " '82. ARTsouthAFRICA (@artsouthafrica)',\n",
       " '81. Kahn Morbee (@KahnMorbee)',\n",
       " '80. Jamal Osman (@JamalMOsman)',\n",
       " '79. iamsuedeâ„¢ (@iamsuede)',\n",
       " '78. Mike Stopforth (@mikestopforth)',\n",
       " '77. Equal Education (@equal_education)',\n",
       " '76. Tristan McConnell (@t_mcconnell)',\n",
       " '75. Kate Forbes (@forbeesta)',\n",
       " '74. Vanessa Raphaely (@hurricanevaness)',\n",
       " '73. Karen Allen (@BBCKarenAllen)',\n",
       " '72. Jax Panik (@jaxpanik)',\n",
       " '71. This Is Africa (@thisisafrica)',\n",
       " '70. Audi South Africa (@audisouthafrica)',\n",
       " '69. ONE (@ONEinAfrica)',\n",
       " '68. Hamza Mohamed (@Hamza_Africa)',\n",
       " '67. drew hinshaw (@drewfhinshaw)',\n",
       " '66. Rebecca Enonchong (@africatechie)',\n",
       " '65. marais (@cx73)',\n",
       " '64. George Ayittey (@ayittey)',\n",
       " '63. Mercedes-Benz SA (@MercedesBenz_SA)',\n",
       " '62. Africa Gathering (@africagathering)',\n",
       " '61. okayafrica (@okayafrica)',\n",
       " '60. Mary Harper (@mary_harper)',\n",
       " '59. Save the Rhino (@savetherhino)',\n",
       " '58. africa @pressfreedom (@africamedia_CPJ)',\n",
       " '57. TechCentral (@TechCentral)',\n",
       " '56. GautengGovt (@GautengProvince)',\n",
       " '55. Abdi Aynte (@Aynte)',\n",
       " '54. Daniel Howden (@daniel_howden)',\n",
       " '53. Ranger Diaries (@rangerdiaries)',\n",
       " '52. The Star (@TheStar_news)',\n",
       " '51. James Schneider (@schneiderhome)',\n",
       " '50. Afrinnovator (@Afrinnovator)',\n",
       " '49. theafricareport (@theafricareport)',\n",
       " '48. City of Joburg (@CityofJoburgZA)',\n",
       " '47. Think Africa Press (@ThinkAfricaFeed)',\n",
       " '46. Africa The Good News (@AfricaGoodNews)',\n",
       " '45. will ross (@willintune)',\n",
       " '44. CNBC Africa (@cnbcafrica)',\n",
       " '43. HowWeMadeItInAfrica (@MadeItInAfrica)',\n",
       " '42. Africa Research Inst (@AfricaResearch)',\n",
       " '41. FoodBlog Cape Town (@FoodBlogCT)',\n",
       " '40. Mbuyiseni Ndlozi (@MbuyiseniNdlozi)',\n",
       " '39. AfricaProgressPanel (@africaprogress)',\n",
       " '38. IFC Africa (@IFCAfrica)',\n",
       " '37. Henley Africa (@HenleyAfrica)',\n",
       " '36. The New Age (@The_New_Age)',\n",
       " '35. Geoffrey York (@geoffreyyork)',\n",
       " '34. Entrepreneur Mag SA (@Entrepreneur_SA)',\n",
       " '33. Forbes Africa (@forbesafrica)',\n",
       " '32. IEC South Africa (@IECSouthAfrica)',\n",
       " '31. Arthur Goldstuck (@art2gee)',\n",
       " '30. Jendayi E Frazer (@JendayiFrazer)',\n",
       " '29. Peter Greste (@PeterGreste)',\n",
       " '28. Disaster Operations (@NDOCKenya)',\n",
       " '27. Mo Ibrahim Fdn (@Mo_IbrahimFdn)',\n",
       " '26. Parliament of RSA (@ParliamentofRSA)',\n",
       " '25. Sandton City (@SandtonCity)',\n",
       " '24. African Union (@_AfricanUnion)',\n",
       " '23. Gert-Johan Coetzee (@gertjohan)',\n",
       " '22. David Smith (@SmithInAfrica)',\n",
       " '21. Ray Hartley (@hartleyr)',\n",
       " '20. Live Amp (@liveamp)',\n",
       " '19. Samsung South Africa (@SamsungSA)',\n",
       " '18. Bob Skinstad (@BobSkinstad)',\n",
       " '17. Camfed (@Camfed)',\n",
       " '16. andrew harding (@BBCAndrewH)',\n",
       " '15. Euphonikâ„¢â™› (@euphonik)',\n",
       " '14. Ulrich J van Vuuren (@UlrichJvV)',\n",
       " '13. John Robbie (@702JohnRobbie)',\n",
       " '12. Cricket South Africa (@OfficialCSA)',\n",
       " '11. MTV Base Africa (@MTVbaseAfrica)',\n",
       " '10. Computicket (@Computicket)',\n",
       " '9. loyiso gola (@loyisogola)',\n",
       " '8. 5FM (@5FM)',\n",
       " '7. mailandguardian (@mailandguardian)',\n",
       " '6. Helen Zille (@helenzille)',\n",
       " '5. Julius Sello Malema (@Julius_S_Malema)',\n",
       " '4. News24 (@News24)',\n",
       " '3. Jacob G. Zuma (@SAPresident)',\n",
       " '2. Gareth Cliff (@GarethCliff)',\n",
       " '1. Trevor Noah (@Trevornoah)',\n",
       " 'Subscribe to the list',\n",
       " 'Tweet about Africa?',\n",
       " 'Celebrate Wild Africa With Us!',\n",
       " 'You have successfully subscribed. Thank you!',\n",
       " '11 Comments']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = get_elements('https://africafreak.com/100-most-influential-twitter-users-in-africa',tag='h2')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  List the Twitter handles of the 10 most influential Twitter users in Africa in order of their popularity (most influential to least influential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" '10. Computicket (@Computicket)'\",\n",
       " \" '9. loyiso gola (@loyisogola)'\",\n",
       " \" '8. 5FM (@5FM)'\",\n",
       " \" '7. mailandguardian (@mailandguardian)'\",\n",
       " \" '6. Helen Zille (@helenzille)'\",\n",
       " \" '5. Julius Sello Malema (@Julius_S_Malema)'\",\n",
       " \" '4. News24 (@News24)'\",\n",
       " \" '3. Jacob G. Zuma (@SAPresident)'\",\n",
       " \" '2. Gareth Cliff (@GarethCliff)'\",\n",
       " \" '1. Trevor Noah (@Trevornoah)'\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_str = str(res)\n",
    "list_str_split = list_str.split(\"11\")\n",
    "new_list = list_str_split[1]\n",
    "new_list\n",
    "new_list2 = new_list.split(\",\")\n",
    "new_list2\n",
    "Influentials = []\n",
    "for x in range(1, 11):\n",
    "    Influentials.append(new_list2[x])\n",
    "Influentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" '10. Computicket (@Computicket)'\",\n",
       " \" '9. loyiso gola (@loyisogola)'\",\n",
       " \" '8. 5FM (@5FM)'\",\n",
       " \" '7. mailandguardian (@mailandguardian)'\",\n",
       " \" '6. Helen Zille (@helenzille)'\",\n",
       " \" '5. Julius Sello Malema (@Julius_S_Malema)'\",\n",
       " \" '4. News24 (@News24)'\",\n",
       " \" '3. Jacob G. Zuma (@SAPresident)'\",\n",
       " \" '2. Gareth Cliff (@GarethCliff)'\",\n",
       " \" '1. Trevor Noah (@Trevornoah)'\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Influentials.reverse()\n",
    "Influentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@Computicket)'\",\n",
       " \"@loyisogola)'\",\n",
       " \"@5FM)'\",\n",
       " \"@mailandguardian)'\",\n",
       " \"@helenzille)'\",\n",
       " \"@Julius_S_Malema)'\",\n",
       " \"@News24)'\",\n",
       " \"@SAPresident)'\",\n",
       " \"@GarethCliff)'\",\n",
       " \"@Trevornoah)'\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Influentials[:5]\n",
    "Influentials = pd.Series(Influentials)\n",
    "user_twitter_handle = [i.split('(')[-1].strip(\")\") for i in Influentials]\n",
    "user_twitter_handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Top 10 Influentials\n",
      "0      @Computicket)'\n",
      "1       @loyisogola)'\n",
      "2              @5FM)'\n",
      "3  @mailandguardian)'\n",
      "4       @helenzille)'\n",
      "5  @Julius_S_Malema)'\n",
      "6           @News24)'\n",
      "7      @SAPresident)'\n",
      "8      @GarethCliff)'\n",
      "9       @Trevornoah)'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pandas import DataFrame\n",
    "df = DataFrame (user_twitter_handle,columns=['Top 10 Influentials'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'tijesunimiolashore@gmail.com_influentials.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Twitter handles of the 10 most influential government officials in Africa in order of their influence (most influential to least influential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = simple_get(url)\n",
    "\n",
    "res = get_elements(response, search={'find_all':{'class_':'wp-block-embed__wrapper'}})\n",
    "res_gov = get_element(response, search={'find_all':{'class_':'wp-block-embed__wrapper'}}) \n",
    "res_gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the list of the 36 government officials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eswatini Government : @EswatiniGovern1\n",
      "Malawi Government : @MalawiGovt\n",
      "Hage G. Geingob : @hagegeingob\n",
      "Seychelles Ministry of Finance : @FinanceSC\n",
      "PresidencyZA : @PresidencyZA\n",
      "Ministry of Health Zambia : @mohzambia\n",
      "President of Zimbabwe : @edmnangagwa\n",
      "MinSantÃ©dj : @MinSantedj\n",
      "Yemane G. Meskel : @hawelti\n",
      "State House Kenya : @StateHouseKenya\n",
      "Paul Kagame : @PaulKagame\n",
      "Mohamed Farmaajo : @M_Farmaajo\n",
      "H.E Hussein Abdelbagi Akol : @SouthSudanGov\n",
      "Abdalla Hamdok : @SudanPMHamdok\n",
      "TanzaniaSpokesperson : @TZSpokesperson\n",
      "Yoweri K Museveni : @KagutaMuseveni\n",
      "MOFA/MRE -(Angola) : @angola_Mirex\n",
      "Amb. Willy Nyamitwe : @willynyamitwe\n",
      "ChÃ©rif Mahamat Zene : @Cherif_MZ\n",
      "PrÃ©sidence RDC ðŸ‡¨ðŸ‡© : @Presidence_RDC\n",
      "Ali Bongo Ondimba : @PresidentABO\n",
      "PrÃ©sidence du BÃ©nin : @PresidenceBenin\n",
      "Roch KABORE : @rochkaborepf\n",
      "Presidente Cabo Verde : @PresidenciaCV\n",
      "Alassane Ouattara : @AOuattara_PRCI\n",
      "State House of The Gambia : @Presidency_GMB\n",
      "Nana Akufo-Addo : @NAkufoAddo\n",
      "Pr. Alpha CONDÃ‰ : @President_GN\n",
      "Umaro Sissoco Embalo : @USEmbalo\n",
      "Presidence Mali : @PresidenceMali\n",
      "Mohamed Cheikh El Ghazouani Ù…Ø­Ù…Ø¯ Ø§Ù„Ø´ÙŠØ® Ø§Ù„ØºØ²ÙˆØ§Ù†ÙŠ : @CheikhGhazouani\n",
      "Issoufou Mahamadou : @IssoufouMhm\n",
      "Muhammadu Buhari : @MBuhari\n",
      "Macky Sall : @Macky_Sall\n",
      "President Julius Maada Bio : @PresidentBio\n",
      "MinistÃ¨re de la SantÃ© et de l'hygiÃ¨ne Publique : @MSPS_Togo\n"
     ]
    }
   ],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = get(url).content\n",
    "res_gov = get_elements(response, tag='blockquote')\n",
    "for r in res_gov:\n",
    "    split_data = r.split('â€” ',maxsplit=1)[1].rsplit('(',maxsplit=1)\n",
    "    name = split_data[0].split(',')[0].strip()\n",
    "    handle =  split_data[1].rsplit(')',maxsplit=1)[0]\n",
    "    print(f'{name} : {handle}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the twitter handles only of the first 10 government officials, we need to get their followers count and deduce the first 10 most influential government officials in Africa from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa'\n",
    "response = get(url).content\n",
    "res_gov = get_elements(response, tag='blockquote')\n",
    "Gov_official_handles = []\n",
    "for r in res_gov:\n",
    "    split_data = r.split('â€” ',maxsplit=1)[1].rsplit('(',maxsplit=1)\n",
    "    name = split_data[0].split(',')[0].strip()\n",
    "    handle =  split_data[1].rsplit(')',maxsplit=1)[0]\n",
    "    Gov_official_handles.append(f'{handle}')\n",
    "\n",
    "    MyList = []\n",
    "for x in range(0, 10):\n",
    "    name = Gov_official_handles[x]\n",
    "    MyList.append(Gov_official_handles[x])\n",
    "MyList\n",
    "\n",
    "import csv\n",
    "from pandas import DataFrame\n",
    "df = DataFrame (MyList,columns=['Top 10 Government Influentials'])\n",
    "df.to_csv(r'tijesunimiolashore@gmail.com_govinfluentials.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in res_gov:\n",
    "    split_data = r.split('â€” ',maxsplit=1)[1].rsplit('(',maxsplit=1)\n",
    "    name = split_data[0].split(',')[0].strip()\n",
    "    handle =  split_data[1].rsplit(')',maxsplit=1)[0]\n",
    "    print(f'{name} : {handle}')\n",
    "    \n",
    "    import tweepy\n",
    "    consumer_key = \"cd6xXCBVFRKohny14KUvgBcyf\"\n",
    "    consumer_secret = \"FvHm7Cd2g73FdZ269VjbZSE3ymngOse3p8RXfpavwbmovupt8d\"\n",
    "    access_token = \"1246485711282462722-8JMUgu4vunrqXs04hfASXgZ1rhP1f3\"\n",
    "    access_token_secret = \"aLi3JSDTZ79XDutDFXiSVU9kqrm88TKNUZF9bG7pIskoB\"\n",
    "\n",
    "    # Creating the authentication object\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    # Setting your access token and secret\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    # Creating the API object while passing in auth information\n",
    "    api = tweepy.API(auth) \n",
    "\n",
    "    for x in range(0, 36):\n",
    "\n",
    "        # The Twitter user who we want to get tweets from\n",
    "        name = Gov_official_handles[x]\n",
    "        # Number of tweets to pull\n",
    "        tweetCount = 10\n",
    "\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        results = api.user_timeline(id=name, count=tweetCount)\n",
    "\n",
    "        # foreach through all tweets pulled\n",
    "        for tweet in results:\n",
    "           # printing the text stored inside the tweet object\n",
    "           print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the top 5 unique hashtags these that the top influencer used in their top 10 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = \"cd6xXCBVFRKohny14KUvgBcyf\"\n",
    "consumer_secret = \"FvHm7Cd2g73FdZ269VjbZSE3ymngOse3p8RXfpavwbmovupt8d\"\n",
    "access_token = \"1246485711282462722-8JMUgu4vunrqXs04hfASXgZ1rhP1f3\"\n",
    "access_token_secret = \"aLi3JSDTZ79XDutDFXiSVU9kqrm88TKNUZF9bG7pIskoB\"\n",
    "\n",
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth) \n",
    "\n",
    "for x in range(0, 10):\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = \"@Trevornoah\"\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 10\n",
    "\n",
    "    # Calling the user_timeline function with our parameters\n",
    "    results = api.user_timeline(id=name, count=tweetCount)\n",
    "\n",
    "    # foreach through all tweets pulled\n",
    "    for tweet in results:\n",
    "       # printing the text stored inside the tweet object\n",
    "       print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the top 5 unique hashtags these that the top government official used in their top 10 retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Twitter user who we want to get tweets from\n",
    "name = \"gettleman\"\n",
    "# Number of tweets to pull\n",
    "tweetCount = 20\n",
    "\n",
    "# Calling the user_timeline function with our parameters\n",
    "results = api.user_timeline(id=name, count=tweetCount)\n",
    "\n",
    "# foreach through all tweets pulled\n",
    "for tweet in results:\n",
    "   # printing the text stored inside the tweet object\n",
    "   print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consumer_key = \"cd6xXCBVFRKohny14KUvgBcyf\"\n",
    "consumer_secret = \"FvHm7Cd2g73FdZ269VjbZSE3ymngOse3p8RXfpavwbmovupt8d\"\n",
    "access_token = \"1246485711282462722-8JMUgu4vunrqXs04hfASXgZ1rhP1f3\"\n",
    "access_token_secret = \"aLi3JSDTZ79XDutDFXiSVU9kqrm88TKNUZF9bG7pIskoB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping using bash script\n",
    "If the web site has a quite simple HTML, you can easily use curl to perform the request and then extract the needed values using bash commands grep, cut , sed, ..\n",
    "\n",
    "This tutorial is adapted from [this](https://medium.com/@LiliSousa/web-scraping-with-bash-690e4ee7f98d) medium article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# curl the page and save content to tmp_file\n",
    "#url = \"https://www.atlanticcouncil.org/blogs/africasource/african-leaders-respond-to-coronavirus-on-twitter/#east-africa\"\n",
    "#curl -X GET $url -o tmp_file\n",
    "\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# write headers to CSV file\n",
    "echo \"Name, twitter_id\" >> extractData.csv\n",
    "n=\"1\"\n",
    "while [ $n -lt 2 ]\n",
    "do\n",
    "  \n",
    "  #get title\n",
    "  title=$(cat tmp_file | grep \"class=\\\"twitter-tweet\\\"\" | cut -d ';' -f1 )\n",
    "  echo $title\n",
    "  #get author\n",
    "  #twitter_id=$(cat tmp_file |grep -A1 \"class=\\\"css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0\\\"\" | tail -1)\n",
    "\n",
    "  #echo \"$title, $twitter_id\" >> extractData.csv\n",
    "  #echo \"$title, $twitter_id\"\n",
    "    \n",
    "  n=$[$n+1]\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
